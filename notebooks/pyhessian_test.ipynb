{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 13:07:14.810293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-05 13:07:14.840390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-05 13:07:14.840426: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-05 13:07:16.132410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/h/ekansh/condaenvs/jax-0.4.23/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/h/ekansh/condaenvs/jax-0.4.23/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m common_utils\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_collections\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_pipeline\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../data/input_pipeline.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/tensorflow/__init__.py:462\u001b[0m\n\u001b[1;32m    460\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras.src.optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m    464\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/pkgs/anaconda39/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/api/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/api/activations/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/activations/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tanh\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[1;32m     25\u001b[0m ALL_OBJECTS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m     relu,\n\u001b[1;32m     27\u001b[0m     leaky_relu,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     log_softmax,\n\u001b[1;32m     44\u001b[0m }\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/saving/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_registered_object\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_keras_serializable\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/saving/saving_api.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy_h5_format\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/legacy/saving/legacy_h5_format.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_utils\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_options\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/legacy/saving/saving_utils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_module\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/models/functional.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization \u001b[38;5;28;01mas\u001b[39;00m legacy_serialization\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _build_map\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/models/model.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable_mapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_saveable_variables\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer \u001b[38;5;28;01mas\u001b[39;00m base_trainer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/trainers/trainer.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileLoss\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileMetrics\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracking\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/trainers/data_adapters/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_data_adapter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_dataset_adapter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_data_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayDataAdapter\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/trainers/data_adapters/array_data_adapter.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_slicing\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAdapter\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/keras/src/trainers/data_adapters/array_slicing.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/pandas/core/api.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     48\u001b[0m     Grouper,\n\u001b[1;32m     49\u001b[0m     NamedAgg,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     52\u001b[0m     CategoricalIndex,\n\u001b[1;32m     53\u001b[0m     DatetimeIndex,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     bdate_range,\n\u001b[1;32m     63\u001b[0m     date_range,\n\u001b[1;32m     64\u001b[0m )\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/pandas/core/groupby/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     DataFrameGroupBy,\n\u001b[1;32m      3\u001b[0m     NamedAgg,\n\u001b[1;32m      4\u001b[0m     SeriesGroupBy,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/pandas/core/groupby/generic.py:60\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     isna,\n\u001b[1;32m     56\u001b[0m     notna,\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m     GroupByApply,\n\u001b[1;32m     62\u001b[0m     maybe_mangle_lambdas,\n\u001b[1;32m     63\u001b[0m     reconstruct_func,\n\u001b[1;32m     64\u001b[0m     validate_func_kwargs,\n\u001b[1;32m     65\u001b[0m     warn_alias_replacement,\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcom\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/pandas/core/apply.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m option_context\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlockValuesRefs\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     AggFuncType,\n\u001b[1;32m     25\u001b[0m     AggFuncTypeBase,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     npt,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_optional_dependency\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from absl import logging\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import jax_utils\n",
    "from flax.training import common_utils\n",
    "\n",
    "import ml_collections\n",
    "\n",
    "from data import input_pipeline\n",
    "import models\n",
    "\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from configs.eval_merge import get_config\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import restore_checkpoint\n",
    "\n",
    "import merging\n",
    "import models\n",
    "\n",
    "\n",
    "from trainer import compute_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(metrics):\n",
    "  return common_utils.stack_forest(metrics)\n",
    "\n",
    "  \n",
    "@functools.partial(jax.jit, static_argnums=(0,))\n",
    "def eval_step(apply_fn, params, batch):\n",
    "    variables = {'params': params}\n",
    "    logits = apply_fn(variables, batch['image'])\n",
    "    metrics = compute_metrics(logits, batch['label'])  \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_loss_and_accuracy(params, apply_fn, dataset, nbatches=None):\n",
    "  eval_iter = input_pipeline.prefetch(dataset, 10, None)\n",
    "  eval_metrics = []\n",
    "  ix = 0\n",
    "  for eval_batch in eval_iter:\n",
    "      metrics = eval_step(apply_fn, params, eval_batch)\n",
    "      eval_metrics.append(metrics)\n",
    "      ix+=1\n",
    "      if nbatches is not None:\n",
    "          if ix >= nbatches:\n",
    "              break\n",
    "  \n",
    "  eval_metrics = get_metrics(eval_metrics)\n",
    "  summary = {\n",
    "          f'eval_{k}': v\n",
    "          for k, v in jax.tree_util.tree_map(\n",
    "              lambda x: x.mean(), eval_metrics\n",
    "          ).items()\n",
    "      }\n",
    "\n",
    "  return summary['eval_loss'], summary['eval_accuracy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(f'VGG16,cifar10.cifar100.eurosat.resisc45.stanford_dogs,mgda-merging')\n",
    "config.width_multiplier = 2\n",
    "config.cifar10.model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/cifar10_96/sgd/epochs_40_0/cosine_1e-2/seed_0/12980603/'\n",
    "config.cifar10.init_model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/cifar10_96/sgd/epochs_40_0/cosine_1e-2/seed_0/12980603/init'\n",
    "config.cifar100.model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/cifar100_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980685'\n",
    "config.cifar100.init_model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/cifar100_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980685/init'\n",
    "config.eurosat.model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/eurosat_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980688'\n",
    "config.eurosat.init_model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/eurosat_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980688/init'\n",
    "config.resisc45.model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/resisc45_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980693/'\n",
    "config.resisc45.init_model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/resisc45_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980693/init'\n",
    "config.stanford_dogs.model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/stanford_dogs_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980694'\n",
    "config.stanford_dogs.init_model_dir = '../checkpoint/VGG16x2/finetuned_imagenet2012_96_13/stanford_dogs_96/sgd/epochs_40_0/cosine_1e-3/seed_0/12980694/init'\n",
    "config.cifar10.pp.crop = 96\n",
    "config.cifar100.pp.crop = 96\n",
    "config.eurosat.pp.crop = 96\n",
    "config.resisc45.pp.crop = 96\n",
    "config.stanford_dogs.pp.crop = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.eval_merge import get_config\n",
    "config = get_config(f'ViTB32,cifar10.cifar100.eurosat.stanford_dogs,mgda-merging')\n",
    "config.cifar10.model_dir = '../experiments/ViT-B-32/cifar10/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.cifar10.init_model_dir = '../experiments/ViT-B-32/cifar10/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.cifar100.model_dir = '../experiments/ViT-B-32/cifar100/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.cifar100.init_model_dir = '../experiments/ViT-B-32/cifar100/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.eurosat.model_dir = '../experiments/ViT-B-32/eurosat/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.eurosat.init_model_dir = '../experiments/ViT-B-32/eurosat/adam_LR_1e-5_WD_1e-1/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.stanford_dogs.model_dir = '../experiments/ViT-B-32/stanford_dogs/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.stanford_dogs.init_model_dir = '../experiments/ViT-B-32/stanford_dogs/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.resisc45.model_dir = '../experiments/ViT-B-32/resisc45/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/'\n",
    "config.resisc45.init_model_dir = '../experiments/ViT-B-32/resisc45/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "\n",
    "config.cifar10.pp.crop = 224\n",
    "config.cifar100.pp.crop = 224\n",
    "config.eurosat.pp.crop = 224\n",
    "config.resisc45.pp.crop = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.eval_merge import get_config\n",
    "config = get_config(f'ViTB32,cifar10.cifar100,mgda-merging')\n",
    "config.cifar10.model_dir = '../experiments/ViT-B-32/cifar10/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/'\n",
    "config.cifar10.init_model_dir = '../experiments/ViT-B-32/cifar10/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.cifar100.model_dir = '../experiments/ViT-B-32/cifar100/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.cifar100.init_model_dir = '../experiments/ViT-B-32/cifar100/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.eurosat.model_dir = '../experiments/ViT-B-32/eurosat/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.eurosat.init_model_dir = '../experiments/ViT-B-32/eurosat/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.stanford_dogs.model_dir = '../experiments/ViT-B-32/stanford_dogs/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0'\n",
    "config.stanford_dogs.init_model_dir = '../experiments/ViT-B-32/stanford_dogs/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "config.resisc45.model_dir = '../experiments/ViT-B-32/resisc45/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/'\n",
    "config.resisc45.init_model_dir = '../experiments/ViT-B-32/resisc45/sgd_LR_1e-3_WD_1e-4/cosine/40_epochs_1_warmup/seed_0/init'\n",
    "\n",
    "config.cifar10.pp.crop = 224\n",
    "config.cifar100.pp.crop = 224\n",
    "config.eurosat.pp.crop = 224\n",
    "config.resisc45.pp.crop = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = config.datasets\n",
    "dataset_info_ls = [input_pipeline.get_dataset_info(dataset, config[dataset].pp['train']) for dataset in config.datasets]\n",
    "num_classes = [dataset_info['num_classes'] for dataset_info in dataset_info_ls] \n",
    "num_train_examples = [dataset_info['num_examples'] for dataset_info in dataset_info_ls]\n",
    "\n",
    "_, ds_test_ls = input_pipeline.get_datasets_for_mtl(config, datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading init and expert models for cifar10.\n",
      "Loading init and expert models for cifar100.\n"
     ]
    }
   ],
   "source": [
    "ntasks = len(num_classes)\n",
    "model_ls = []\n",
    "for nclass in num_classes:\n",
    "    model_ls += [models.create_model(\n",
    "                    model_cls= getattr(models, config.model), \n",
    "                    num_classes=nclass, \n",
    "                    width_multiplier=config.width_multiplier,\n",
    "                    projection_dim=512, \n",
    "                    half_precision=config.half_precision) ]\n",
    "\n",
    "## Get expert + init  models\n",
    "expert_params_ls = []\n",
    "init_params_ls = []\n",
    "for dataset in datasets:\n",
    "    print(f\"Loading init and expert models for {dataset}.\")\n",
    "    expert_params_ls += [restore_checkpoint(config[dataset].model_dir)['params']]\n",
    "    init_params_ls += [restore_checkpoint(config[dataset].init_model_dir)['params']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian import compute_density, compute_trace, compute_eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import jax_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tree_zeros_like, tree_scalar_multiply, tree_add\n",
    "from pyhessian.hvp import compute_batch_hvp\n",
    "\n",
    "\n",
    "def compute_hessian_vector_product(loss_fn, dataset, params, v, nbatches=None, axis_name=None):\n",
    "    eval_iter =  input_pipeline.prefetch(dataset, 10, None)\n",
    "    if axis_name is not None: \n",
    "        batch_hvp = jax.pmap(compute_batch_hvp, static_argnums=(0, 4))\n",
    "    else:\n",
    "        batch_hvp = jax.jit(compute_batch_hvp, static_argnums=(0, 4))\n",
    "    N = 0\n",
    "    Hv = tree_zeros_like(v)\n",
    "    ix = 0\n",
    "    \n",
    "    if axis_name is not None:\n",
    "        N_fn = lambda batch: batch['label'].shape[0] * batch['label'].shape[1]\n",
    "    else:\n",
    "        N_fn = lambda batch: batch['label'].shape[0] \n",
    "    \n",
    "    for batch in eval_iter:\n",
    "        batch_n = N_fn(batch)\n",
    "        N += batch_n\n",
    "        _Hv = batch_hvp(loss_fn, batch, params, v, axis_name)\n",
    "        Hv = tree_add(Hv, tree_scalar_multiply(batch_n, _Hv))\n",
    "        ix+=1\n",
    "        if nbatches is not None:\n",
    "          if ix >= nbatches:\n",
    "              break\n",
    "        \n",
    "    return tree_scalar_multiply(1./N, Hv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_entropy_loss\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(applyfn, batch, params):    \n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m applyfn(\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params},\n\u001b[1;32m      6\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trainer'"
     ]
    }
   ],
   "source": [
    "from trainer import cross_entropy_loss\n",
    "\n",
    "def loss_fn(applyfn, batch, params):    \n",
    "    logits = applyfn(\n",
    "    {'params': params},\n",
    "    batch['image'])\n",
    "    loss = cross_entropy_loss(logits, batch['label'])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hessian_vector_product(lambda batch, params: loss_fn(model_ls[0].apply, batch, ), dataset, params, v, nbatches=None, axis_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HVP = lambda params, v: compute_hessian_vector_product(loss_fn, ds_test_ls[0], params, v, nbatches=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]2024-08-01 13:32:20.688868: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  1%|          | 1/100 [01:49<2:59:58, 109.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6894.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:33:00.448414: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  2%|         | 2/100 [02:26<1:49:08, 66.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6335.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:33:37.419611: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  3%|         | 3/100 [03:03<1:25:55, 53.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6057.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:34:14.251018: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  4%|         | 4/100 [03:40<1:14:43, 46.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:34:50.661338: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  5%|         | 5/100 [04:16<1:08:04, 42.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:35:27.286049: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  6%|         | 6/100 [04:53<1:03:58, 40.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5838.3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 13:36:03.660330: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  6%|         | 6/100 [05:29<1:26:01, 54.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5839.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trace = compute_trace(random.PRNGKey(10),HVP, expert_params_ls[0], tol = 1e-3)\n",
    "print(jnp.mean(jnp.stack(trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(6894.7056, dtype=float32),\n",
       " Array(5776.845, dtype=float32),\n",
       " Array(5500.2686, dtype=float32),\n",
       " Array(4111.1377, dtype=float32),\n",
       " Array(5011.5947, dtype=float32),\n",
       " Array(7735.4004, dtype=float32),\n",
       " Array(5845.1787, dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]2024-08-01 11:33:09.932906: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  1%|          | 1/100 [00:37<1:01:48, 37.46s/it]2024-08-01 11:33:46.667071: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  2%|         | 2/100 [01:14<1:00:29, 37.03s/it]2024-08-01 11:34:22.930463: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  3%|         | 3/100 [01:50<59:18, 36.68s/it]  2024-08-01 11:35:00.130878: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  4%|         | 4/100 [02:27<59:01, 36.89s/it]2024-08-01 11:35:37.170383: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  5%|         | 5/100 [03:04<58:29, 36.94s/it]2024-08-01 11:36:13.810336: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  6%|         | 6/100 [03:41<57:49, 36.91s/it]2024-08-01 11:36:50.079180: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  6%|         | 6/100 [04:17<1:07:18, 42.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5832.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trace = compute_trace(random.PRNGKey(10),HVP, expert_params_ls[0], tol=1e-3)\n",
    "print(jnp.mean(jnp.stack(trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(6886.1846, dtype=float32),\n",
       " Array(5770.9224, dtype=float32),\n",
       " Array(5494.73, dtype=float32),\n",
       " Array(4103.216, dtype=float32),\n",
       " Array(5003.029, dtype=float32),\n",
       " Array(7730.3623, dtype=float32),\n",
       " Array(5839.726, dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]2024-08-01 11:50:47.787549: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "  1%|          | 1/100 [00:42<1:09:54, 42.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trace \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHVP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params_ls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39mmean(jnp\u001b[38;5;241m.\u001b[39mstack(trace)))\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../pyhessian/pyhessian.py:68\u001b[0m, in \u001b[0;36mcompute_trace\u001b[0;34m(rng, hvp_fn, params, max_iter, tol)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(max_iter)):\n\u001b[1;32m     67\u001b[0m     rad_v \u001b[38;5;241m=\u001b[39m rademacher_tree_like(rngs[ix], params) \u001b[38;5;66;03m# generate Rademacher random variables \u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     Hv \u001b[38;5;241m=\u001b[39m  \u001b[43mhvp_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrad_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     trace_vhv \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [tree_inner_prod(rad_v, Hv)]\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(jnp\u001b[38;5;241m.\u001b[39mmean(jnp\u001b[38;5;241m.\u001b[39mstack(trace_vhv)) \u001b[38;5;241m-\u001b[39m trace) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mabs\u001b[39m(trace) \u001b[38;5;241m+\u001b[39m TINY) \u001b[38;5;241m<\u001b[39m tol:\n",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(params, v)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m HVP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m params, v: \u001b[43mcompute_hessian_vector_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_test_ls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mcompute_hessian_vector_product\u001b[0;34m(loss_fn, dataset, params, v, nbatches, axis_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m N \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_n\n\u001b[1;32m     23\u001b[0m _Hv \u001b[38;5;241m=\u001b[39m batch_hvp(loss_fn, batch, params, v, axis_name)\n\u001b[0;32m---> 24\u001b[0m Hv \u001b[38;5;241m=\u001b[39m tree_add(Hv, \u001b[43mtree_scalar_multiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Hv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m ix\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nbatches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../utils.py:70\u001b[0m, in \u001b[0;36mtree_scalar_multiply\u001b[0;34m(c, t)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_scalar_multiply\u001b[39m(c, t):\n\u001b[0;32m---> 70\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/tree_util.py:244\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    243\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/tree_util.py:244\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    243\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../utils.py:70\u001b[0m, in \u001b[0;36mtree_scalar_multiply.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_scalar_multiply\u001b[39m(c, t):\n\u001b[0;32m---> 70\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mc\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m , t)\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/numpy/array_methods.py:271\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    269\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trace = compute_trace(random.PRNGKey(10),HVP, init_params_ls[0], tol=1e-3)\n",
    "print(jnp.mean(jnp.stack(trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]2024-07-31 13:29:22.248265: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  1%|          | 1/100 [00:06<11:08,  6.76s/it]2024-07-31 13:29:28.856219: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  2%|         | 2/100 [00:13<10:53,  6.67s/it]2024-07-31 13:29:35.441183: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  3%|         | 3/100 [00:19<10:43,  6.63s/it]2024-07-31 13:29:42.070197: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  4%|         | 4/100 [00:26<10:36,  6.63s/it]2024-07-31 13:29:48.649419: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 5/100 [00:33<10:28,  6.61s/it]2024-07-31 13:29:55.261460: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  6%|         | 6/100 [00:39<10:21,  6.61s/it]2024-07-31 13:30:01.878736: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  7%|         | 7/100 [00:46<10:15,  6.62s/it]2024-07-31 13:30:08.509767: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  8%|         | 8/100 [00:53<10:09,  6.62s/it]2024-07-31 13:30:15.126481: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  9%|         | 9/100 [00:59<10:02,  6.62s/it]2024-07-31 13:30:21.726091: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 10%|         | 10/100 [01:06<09:55,  6.61s/it]2024-07-31 13:30:28.355431: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 11/100 [01:12<09:48,  6.62s/it]2024-07-31 13:30:35.007124: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 12%|        | 12/100 [01:19<09:43,  6.63s/it]2024-07-31 13:30:41.615337: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 13%|        | 13/100 [01:26<09:35,  6.62s/it]2024-07-31 13:30:48.188481: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 14%|        | 14/100 [01:32<09:28,  6.61s/it]2024-07-31 13:30:54.759775: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 15%|        | 15/100 [01:39<09:20,  6.60s/it]2024-07-31 13:31:01.394673: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 16/100 [01:45<09:15,  6.61s/it]2024-07-31 13:31:08.101736: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 17%|        | 17/100 [01:52<09:10,  6.64s/it]2024-07-31 13:31:14.655427: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 18%|        | 18/100 [01:59<09:02,  6.61s/it]2024-07-31 13:31:21.269152: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 19%|        | 19/100 [02:05<08:55,  6.61s/it]2024-07-31 13:31:27.918340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 20%|        | 20/100 [02:12<08:54,  6.68s/it]2024-07-31 13:31:34.758276: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 21/100 [02:19<08:51,  6.73s/it]2024-07-31 13:31:41.518136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 22%|       | 22/100 [02:26<08:45,  6.73s/it]2024-07-31 13:31:48.277901: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 23%|       | 23/100 [02:32<08:39,  6.74s/it]2024-07-31 13:31:54.989881: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 24%|       | 24/100 [02:39<08:31,  6.74s/it]2024-07-31 13:32:01.760220: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 25%|       | 25/100 [02:46<08:26,  6.75s/it]2024-07-31 13:32:08.517254: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 26/100 [02:53<08:19,  6.75s/it]2024-07-31 13:32:15.279999: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 27%|       | 27/100 [02:59<08:12,  6.75s/it]2024-07-31 13:32:22.052568: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 28%|       | 28/100 [03:06<08:06,  6.76s/it]2024-07-31 13:32:28.822895: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 29%|       | 29/100 [03:13<08:00,  6.76s/it]2024-07-31 13:32:35.643425: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 30%|       | 30/100 [03:20<07:54,  6.78s/it]2024-07-31 13:32:42.387559: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 31%|       | 31/100 [03:27<07:46,  6.77s/it]2024-07-31 13:32:49.126008: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 32/100 [03:33<07:39,  6.76s/it]2024-07-31 13:32:55.907201: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 33%|      | 33/100 [03:40<07:33,  6.77s/it]2024-07-31 13:33:02.675519: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 34%|      | 34/100 [03:47<07:26,  6.77s/it]2024-07-31 13:33:09.458535: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 35%|      | 35/100 [03:54<07:20,  6.77s/it]2024-07-31 13:33:16.226575: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 36%|      | 36/100 [04:00<07:13,  6.77s/it]2024-07-31 13:33:23.019795: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 37/100 [04:07<07:06,  6.78s/it]2024-07-31 13:33:29.828026: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 38%|      | 38/100 [04:14<07:00,  6.79s/it]2024-07-31 13:33:36.697502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 39%|      | 39/100 [04:21<06:55,  6.81s/it]2024-07-31 13:33:43.453584: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 40%|      | 40/100 [04:28<06:47,  6.79s/it]2024-07-31 13:33:50.231395: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 41%|      | 41/100 [04:34<06:40,  6.79s/it]2024-07-31 13:33:56.990223: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 42/100 [04:41<06:33,  6.78s/it]2024-07-31 13:34:03.791234: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 43%|     | 43/100 [04:48<06:26,  6.79s/it]2024-07-31 13:34:10.642139: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 44%|     | 44/100 [04:55<06:21,  6.81s/it]2024-07-31 13:34:17.502419: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 45%|     | 45/100 [05:02<06:15,  6.82s/it]2024-07-31 13:34:24.263256: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 46%|     | 46/100 [05:08<06:07,  6.80s/it]2024-07-31 13:34:31.263310: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 47/100 [05:15<06:03,  6.86s/it]2024-07-31 13:34:38.042084: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 48%|     | 48/100 [05:22<05:55,  6.84s/it]2024-07-31 13:34:44.827032: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 49%|     | 49/100 [05:29<05:47,  6.82s/it]2024-07-31 13:34:51.593902: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 50%|     | 50/100 [05:36<05:40,  6.81s/it]2024-07-31 13:34:58.377854: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 51%|     | 51/100 [05:43<05:33,  6.80s/it]2024-07-31 13:35:05.144528: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 52%|    | 52/100 [05:49<05:25,  6.79s/it]2024-07-31 13:35:11.929338: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 53/100 [05:56<05:19,  6.79s/it]2024-07-31 13:35:18.712744: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 54%|    | 54/100 [06:03<05:12,  6.79s/it]2024-07-31 13:35:25.589530: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 55%|    | 55/100 [06:10<05:06,  6.81s/it]2024-07-31 13:35:32.371743: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 56%|    | 56/100 [06:17<04:59,  6.80s/it]2024-07-31 13:35:39.217296: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 57%|    | 57/100 [06:23<04:52,  6.81s/it]2024-07-31 13:35:45.973615: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 58/100 [06:30<04:45,  6.80s/it]2024-07-31 13:35:52.745154: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 59%|    | 59/100 [06:37<04:38,  6.79s/it]2024-07-31 13:35:59.490289: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 60%|    | 60/100 [06:44<04:31,  6.78s/it]2024-07-31 13:36:06.282967: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 61%|    | 61/100 [06:51<04:24,  6.78s/it]2024-07-31 13:36:13.058064: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 62%|   | 62/100 [06:57<04:17,  6.78s/it]2024-07-31 13:36:19.867975: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 63/100 [07:04<04:11,  6.79s/it]2024-07-31 13:36:26.743908: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 64%|   | 64/100 [07:11<04:05,  6.82s/it]2024-07-31 13:36:33.523940: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 65%|   | 65/100 [07:18<03:58,  6.80s/it]2024-07-31 13:36:40.350721: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 66%|   | 66/100 [07:25<03:51,  6.81s/it]2024-07-31 13:36:47.159285: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 67%|   | 67/100 [07:31<03:44,  6.81s/it]2024-07-31 13:36:54.004047: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 68/100 [07:38<03:38,  6.82s/it]2024-07-31 13:37:00.828216: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 69%|   | 69/100 [07:49<04:05,  7.93s/it]2024-07-31 13:37:11.328152: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 70%|   | 70/100 [07:56<03:47,  7.60s/it]2024-07-31 13:37:18.134900: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 71%|   | 71/100 [08:02<03:33,  7.36s/it]2024-07-31 13:37:24.929575: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 72%|  | 72/100 [08:09<03:21,  7.19s/it]2024-07-31 13:37:31.763708: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 73%|  | 73/100 [08:16<03:11,  7.08s/it]2024-07-31 13:37:38.608259: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 74/100 [08:23<03:02,  7.01s/it]2024-07-31 13:37:45.463360: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 75%|  | 75/100 [08:30<02:54,  6.96s/it]2024-07-31 13:37:52.303671: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 76%|  | 76/100 [08:37<02:46,  6.93s/it]2024-07-31 13:37:59.145978: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 77%|  | 77/100 [08:43<02:38,  6.90s/it]2024-07-31 13:38:05.962035: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 78%|  | 78/100 [08:50<02:31,  6.87s/it]2024-07-31 13:38:12.978267: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 79/100 [08:57<02:25,  6.92s/it]2024-07-31 13:38:19.847735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 80%|  | 80/100 [09:04<02:18,  6.90s/it]2024-07-31 13:38:26.664168: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 81%|  | 81/100 [09:11<02:10,  6.88s/it]2024-07-31 13:38:33.489775: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 82%| | 82/100 [09:18<02:03,  6.86s/it]2024-07-31 13:38:40.318235: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 83%| | 83/100 [09:25<01:56,  6.85s/it]2024-07-31 13:38:47.133144: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 84/100 [09:35<02:07,  7.96s/it]2024-07-31 13:38:57.668122: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 85%| | 85/100 [09:42<01:54,  7.62s/it]2024-07-31 13:39:04.450439: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 86%| | 86/100 [09:49<01:43,  7.36s/it]2024-07-31 13:39:11.250878: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 87%| | 87/100 [09:55<01:33,  7.19s/it]2024-07-31 13:39:18.012280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 88%| | 88/100 [10:02<01:24,  7.07s/it]2024-07-31 13:39:24.865341: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 89/100 [10:09<01:17,  7.00s/it]2024-07-31 13:39:31.686726: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 90%| | 90/100 [10:16<01:09,  6.95s/it]2024-07-31 13:39:38.499487: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 91%| | 91/100 [10:23<01:02,  6.91s/it]2024-07-31 13:39:45.331566: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 92%|| 92/100 [10:30<00:55,  6.89s/it]2024-07-31 13:39:52.127663: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 93%|| 93/100 [10:36<00:48,  6.86s/it]2024-07-31 13:39:58.927809: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 94%|| 94/100 [10:43<00:41,  6.84s/it]2024-07-31 13:40:05.715112: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 95/100 [10:50<00:34,  6.82s/it]2024-07-31 13:40:12.549081: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 96%|| 96/100 [10:57<00:27,  6.83s/it]2024-07-31 13:40:19.368278: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 97%|| 97/100 [11:04<00:20,  6.82s/it]2024-07-31 13:40:26.177471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 98%|| 98/100 [11:10<00:13,  6.82s/it]2024-07-31 13:40:32.977820: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 99%|| 99/100 [11:17<00:06,  6.82s/it]2024-07-31 13:40:39.783207: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 100/100 [11:24<00:00,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trace = compute_trace(random.PRNGKey(0),HVP, init_params_ls[0])\n",
    "print(jnp.mean(jnp.stack(trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_init = compute_eigenvalues(random.PRNGKey(0),HVP, init_params_ls[0], top_n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_expert = compute_eigenvalues(random.PRNGKey(0),HVP, expert_params_ls[0], top_n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(14053.352, dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_init[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(14053.352, dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_expert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 10:56:39.951517: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:56:42.225184: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:16,  1.06it/s]2024-08-01 10:56:43.007613: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:14,  1.17it/s]2024-08-01 10:56:43.787491: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:13,  1.22it/s]2024-08-01 10:56:44.572568: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:12,  1.24it/s]2024-08-01 10:56:45.363270: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:04<00:11,  1.25it/s]2024-08-01 10:56:46.134544: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.26it/s]2024-08-01 10:56:46.999919: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.22it/s]2024-08-01 10:56:47.801736: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.24it/s]2024-08-01 10:56:48.580064: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.25it/s]2024-08-01 10:56:49.358847: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:08<00:08,  1.03it/s]2024-08-01 10:56:50.749125: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:09<00:07,  1.08it/s]2024-08-01 10:56:51.530739: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:10<00:06,  1.14it/s]2024-08-01 10:56:52.305210: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:11<00:05,  1.18it/s]2024-08-01 10:56:53.075735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:04,  1.21it/s]2024-08-01 10:56:53.851173: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:12<00:03,  1.23it/s]2024-08-01 10:56:54.628879: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:13<00:02,  1.25it/s]2024-08-01 10:56:55.400703: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:14<00:01,  1.26it/s]2024-08-01 10:56:56.172667: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.27it/s]2024-08-01 10:56:56.938111: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:15<00:00,  1.21it/s]\n",
      "2024-08-01 10:56:57.850726: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:56:58.624231: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:13,  1.29it/s]2024-08-01 10:56:59.393376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.30it/s]2024-08-01 10:57:00.171479: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.29it/s]2024-08-01 10:57:00.943069: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.29it/s]2024-08-01 10:57:01.722936: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:10,  1.29it/s]2024-08-01 10:57:02.494471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.29it/s]2024-08-01 10:57:03.282063: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.28it/s]2024-08-01 10:57:04.065223: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.28it/s]2024-08-01 10:57:04.840814: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:06<00:07,  1.29it/s]2024-08-01 10:57:05.611423: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:06,  1.29it/s]2024-08-01 10:57:06.391729: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.29it/s]2024-08-01 10:57:07.165225: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.29it/s]2024-08-01 10:57:07.939063: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.29it/s]2024-08-01 10:57:08.726267: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:10<00:03,  1.28it/s]2024-08-01 10:57:09.510241: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.28it/s]2024-08-01 10:57:10.289070: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.28it/s]2024-08-01 10:57:11.063422: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.29it/s]2024-08-01 10:57:11.838741: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:13<00:00,  1.29it/s]2024-08-01 10:57:12.619921: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:14<00:00,  1.29it/s]\n",
      "2024-08-01 10:57:13.426809: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:57:14.200505: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:13,  1.29it/s]2024-08-01 10:57:14.990377: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.28it/s]2024-08-01 10:57:15.768948: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.28it/s]2024-08-01 10:57:16.557337: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.28it/s]2024-08-01 10:57:17.334984: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:10,  1.28it/s]2024-08-01 10:57:18.110468: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.28it/s]2024-08-01 10:57:18.945304: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.26it/s]2024-08-01 10:57:19.741606: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.25it/s]2024-08-01 10:57:20.523959: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.26it/s]2024-08-01 10:57:21.311371: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.26it/s]2024-08-01 10:57:22.085994: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.27it/s]2024-08-01 10:57:22.872458: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.27it/s]2024-08-01 10:57:23.655462: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.28it/s]2024-08-01 10:57:24.436588: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.27it/s]2024-08-01 10:57:25.212006: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.28it/s]2024-08-01 10:57:25.998384: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.28it/s]2024-08-01 10:57:26.798648: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.27it/s]2024-08-01 10:57:27.576474: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.27it/s]2024-08-01 10:57:28.362340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:14<00:00,  1.27it/s]\n",
      "2024-08-01 10:57:29.170035: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:57:29.941079: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:13,  1.29it/s]2024-08-01 10:57:30.727394: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.29it/s]2024-08-01 10:57:31.505949: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.28it/s]2024-08-01 10:57:32.279020: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.29it/s]2024-08-01 10:57:33.057905: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:10,  1.28it/s]2024-08-01 10:57:33.844422: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.28it/s]2024-08-01 10:57:34.630917: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.28it/s]2024-08-01 10:57:35.413007: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.28it/s]2024-08-01 10:57:36.192819: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.28it/s]2024-08-01 10:57:36.973723: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.28it/s]2024-08-01 10:57:37.760142: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.28it/s]2024-08-01 10:57:38.594176: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.26it/s]2024-08-01 10:57:39.373818: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.26it/s]2024-08-01 10:57:40.151866: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:10<00:03,  1.27it/s]2024-08-01 10:57:40.930820: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.28it/s]2024-08-01 10:57:41.698950: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.28it/s]2024-08-01 10:57:42.472325: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.28it/s]2024-08-01 10:57:43.258059: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.28it/s]2024-08-01 10:57:44.054360: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:14<00:00,  1.28it/s]\n",
      "2024-08-01 10:57:44.853897: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:57:45.629515: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.28it/s]2024-08-01 10:57:46.412128: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.28it/s]2024-08-01 10:57:47.194232: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.27it/s]2024-08-01 10:57:47.994860: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.27it/s]2024-08-01 10:57:48.781340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:11,  1.27it/s]2024-08-01 10:57:49.558136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.28it/s]2024-08-01 10:57:50.338574: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.27it/s]2024-08-01 10:57:51.151686: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.26it/s]2024-08-01 10:57:51.937370: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.27it/s]2024-08-01 10:57:52.715571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.27it/s]2024-08-01 10:57:53.491927: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.28it/s]2024-08-01 10:57:54.275896: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.28it/s]2024-08-01 10:57:55.052341: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.28it/s]2024-08-01 10:57:55.831832: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:10<00:03,  1.28it/s]2024-08-01 10:57:56.619863: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.28it/s]2024-08-01 10:57:57.403538: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.28it/s]2024-08-01 10:57:58.177482: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.28it/s]2024-08-01 10:57:58.957849: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.28it/s]2024-08-01 10:57:59.739320: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:15<00:00,  1.23it/s]\n",
      "2024-08-01 10:58:01.132812: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:58:01.917433: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.26it/s]2024-08-01 10:58:02.704556: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.27it/s]2024-08-01 10:58:03.500859: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.26it/s]2024-08-01 10:58:04.313055: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.25it/s]2024-08-01 10:58:05.095412: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:11,  1.26it/s]2024-08-01 10:58:05.879271: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.27it/s]2024-08-01 10:58:06.673324: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.27it/s]2024-08-01 10:58:07.456459: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.27it/s]2024-08-01 10:58:08.245025: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:09.036280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:09.832091: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.26it/s]2024-08-01 10:58:10.613586: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.26it/s]2024-08-01 10:58:11.405660: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.27it/s]2024-08-01 10:58:12.192774: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.27it/s]2024-08-01 10:58:12.979809: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.27it/s]2024-08-01 10:58:13.765002: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.27it/s]2024-08-01 10:58:14.553280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.27it/s]2024-08-01 10:58:15.353315: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.04it/s]2024-08-01 10:58:16.706101: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:15<00:00,  1.22it/s]\n",
      "2024-08-01 10:58:17.505614: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:58:18.296191: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.26it/s]2024-08-01 10:58:19.069282: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.28it/s]2024-08-01 10:58:19.842072: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.28it/s]2024-08-01 10:58:20.639199: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.28it/s]2024-08-01 10:58:21.419378: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:10,  1.28it/s]2024-08-01 10:58:22.195134: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.28it/s]2024-08-01 10:58:22.972468: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.28it/s]2024-08-01 10:58:23.755710: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.28it/s]2024-08-01 10:58:24.540604: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.28it/s]2024-08-01 10:58:25.324485: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.28it/s]2024-08-01 10:58:26.107521: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.28it/s]2024-08-01 10:58:26.909724: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.27it/s]2024-08-01 10:58:27.710707: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.26it/s]2024-08-01 10:58:28.518389: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.26it/s]2024-08-01 10:58:29.356502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.24it/s]2024-08-01 10:58:30.152668: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.24it/s]2024-08-01 10:58:30.965133: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.24it/s]2024-08-01 10:58:31.787533: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.23it/s]2024-08-01 10:58:32.562163: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:15<00:00,  1.26it/s]\n",
      "2024-08-01 10:58:33.380260: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:58:34.196238: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.22it/s]2024-08-01 10:58:34.977288: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.26it/s]2024-08-01 10:58:35.789298: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.25it/s]2024-08-01 10:58:36.575696: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.26it/s]2024-08-01 10:58:37.360796: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:11,  1.26it/s]2024-08-01 10:58:38.147711: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.27it/s]2024-08-01 10:58:38.929548: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.27it/s]2024-08-01 10:58:39.716775: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.27it/s]2024-08-01 10:58:40.495685: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:41.285130: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:42.101397: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.26it/s]2024-08-01 10:58:42.881941: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.26it/s]2024-08-01 10:58:43.656662: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.27it/s]2024-08-01 10:58:44.436584: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.27it/s]2024-08-01 10:58:45.223142: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.27it/s]2024-08-01 10:58:46.006039: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.27it/s]2024-08-01 10:58:46.784797: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.28it/s]2024-08-01 10:58:47.563092: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.28it/s]2024-08-01 10:58:48.352600: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:14<00:00,  1.27it/s]\n",
      "2024-08-01 10:58:49.157008: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:58:49.946840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.27it/s]2024-08-01 10:58:50.730743: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:01<00:13,  1.27it/s]2024-08-01 10:58:51.520026: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:12,  1.27it/s]2024-08-01 10:58:52.301403: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:11,  1.27it/s]2024-08-01 10:58:53.081871: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:03<00:10,  1.28it/s]2024-08-01 10:58:53.862097: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:04<00:10,  1.27it/s]2024-08-01 10:58:54.669984: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:05<00:09,  1.27it/s]2024-08-01 10:58:55.453247: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.27it/s]2024-08-01 10:58:56.240320: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:57.024089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:07<00:07,  1.27it/s]2024-08-01 10:58:57.801529: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:08<00:06,  1.28it/s]2024-08-01 10:58:58.585967: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:09<00:05,  1.28it/s]2024-08-01 10:58:59.362022: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.28it/s]2024-08-01 10:59:00.163093: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.27it/s]2024-08-01 10:59:00.948242: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:11<00:03,  1.27it/s]2024-08-01 10:59:01.722752: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:12<00:02,  1.28it/s]2024-08-01 10:59:02.500747: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.28it/s]2024-08-01 10:59:03.281325: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.28it/s]2024-08-01 10:59:04.066609: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:14<00:00,  1.27it/s]\n",
      "2024-08-01 10:59:04.861896: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]2024-08-01 10:59:05.634317: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  5%|         | 1/19 [00:00<00:14,  1.28it/s]2024-08-01 10:59:06.413975: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 11%|         | 2/19 [00:02<00:19,  1.13s/it]2024-08-01 10:59:07.785424: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 16%|        | 3/19 [00:02<00:15,  1.03it/s]2024-08-01 10:59:08.560617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 21%|        | 4/19 [00:03<00:13,  1.12it/s]2024-08-01 10:59:09.354922: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 26%|       | 5/19 [00:04<00:11,  1.17it/s]2024-08-01 10:59:10.131976: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 32%|      | 6/19 [00:05<00:10,  1.20it/s]2024-08-01 10:59:10.917647: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 37%|      | 7/19 [00:06<00:09,  1.23it/s]2024-08-01 10:59:11.704828: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 42%|     | 8/19 [00:06<00:08,  1.24it/s]2024-08-01 10:59:12.476195: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 47%|     | 9/19 [00:07<00:07,  1.26it/s]2024-08-01 10:59:13.266238: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 53%|    | 10/19 [00:08<00:07,  1.26it/s]2024-08-01 10:59:14.088432: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 58%|    | 11/19 [00:09<00:06,  1.25it/s]2024-08-01 10:59:14.871246: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 63%|   | 12/19 [00:10<00:05,  1.26it/s]2024-08-01 10:59:15.661954: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 68%|   | 13/19 [00:10<00:04,  1.26it/s]2024-08-01 10:59:16.449072: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 74%|  | 14/19 [00:11<00:03,  1.26it/s]2024-08-01 10:59:17.235285: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 79%|  | 15/19 [00:12<00:03,  1.27it/s]2024-08-01 10:59:18.021452: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 84%| | 16/19 [00:13<00:02,  1.27it/s]2024-08-01 10:59:18.794723: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 89%| | 17/19 [00:13<00:01,  1.28it/s]2024-08-01 10:59:19.571970: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 95%|| 18/19 [00:14<00:00,  1.28it/s]2024-08-01 10:59:20.357942: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "100%|| 19/19 [00:15<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "eigenvals,  = compute_density(random.PRNGKey(0),HVP, expert_params_ls[0], n_eigs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import get_esd_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals , weights = density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (2, 10, 20) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mget_esd_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../plotting.py:28\u001b[0m, in \u001b[0;36mget_esd_plot\u001b[0;34m(eigenvalues, weights, shape, fname)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_esd_plot\u001b[39m(eigenvalues, weights, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m), fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     density, grids \u001b[38;5;241m=\u001b[39m \u001b[43mdensity_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39msemilogy(grids, density \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0e-7\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/ekansh/repos/share/vision-models/notebooks/../plotting.py:48\u001b[0m, in \u001b[0;36mdensity_generate\u001b[0;34m(eigenvalues, weights, num_bins, sigma_squared, overhead)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdensity_generate\u001b[39m(eigenvalues,\n\u001b[1;32m     43\u001b[0m                      weights,\n\u001b[1;32m     44\u001b[0m                      num_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     45\u001b[0m                      sigma_squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     46\u001b[0m                      overhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     eigenvalues \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(weights)\n\u001b[1;32m     51\u001b[0m     lambda_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mmax(eigenvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m overhead\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (2, 10, 20) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "fig = get_esd_plot(eigvals, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_esd_plot(eigenvalues, weights):\n",
    "    density, grids = density_generate(eigenvalues, weights)\n",
    "    plt.semilogy(grids, density + 1.0e-7)\n",
    "    plt.ylabel('Density (Log Scale)', fontsize=14, labelpad=10)\n",
    "    plt.xlabel('Eigenvlaue', fontsize=14, labelpad=10)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.axis([np.min(eigenvalues) - 1, np.max(eigenvalues) + 1, None, None])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig('example.pdf')\n",
    "\n",
    "\n",
    "def density_generate(eigenvalues,\n",
    "                     weights,\n",
    "                     num_bins=10000,\n",
    "                     sigma_squared=1e-2,\n",
    "                     overhead=0.01):\n",
    "\n",
    "    eigenvalues = np.array(eigenvalues)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    lambda_max = np.mean(np.max(eigenvalues, axis=1), axis=0) + overhead\n",
    "    lambda_min = np.mean(np.min(eigenvalues, axis=1), axis=0) - overhead\n",
    "\n",
    "    grids = np.linspace(lambda_min, lambda_max, num=num_bins)\n",
    "    sigma = sigma_squared * max(1, (lambda_max - lambda_min))\n",
    "\n",
    "    num_runs = eigenvalues.shape[0]\n",
    "    density_output = np.zeros((num_runs, num_bins))\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        for j in range(num_bins):\n",
    "            x = grids[j]\n",
    "            tmp_result = gaussian(eigenvalues[i, :], x, sigma)\n",
    "            density_output[i, j] = np.sum(tmp_result * weights[i, :])\n",
    "    density = np.mean(density_output, axis=0)\n",
    "    normalization = np.sum(density) * (grids[1] - grids[0])\n",
    "    density = density / normalization\n",
    "    return density, grids\n",
    "\n",
    "\n",
    "def gaussian(x, x0, sigma_squared):\n",
    "    return np.exp(-(x0 - x)**2 /\n",
    "                  (2.0 * sigma_squared)) / np.sqrt(2 * np.pi * sigma_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_esd_plot(eigvals, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eigvals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m density, grids \u001b[38;5;241m=\u001b[39m \u001b[43mdensity_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 30\u001b[0m, in \u001b[0;36mdensity_generate\u001b[0;34m(eigenvalues, weights, num_bins, sigma_squared, overhead)\u001b[0m\n\u001b[1;32m     27\u001b[0m eigenvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(eigenvalues)\n\u001b[1;32m     28\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(weights)\n\u001b[0;32m---> 30\u001b[0m lambda_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m overhead\n\u001b[1;32m     31\u001b[0m lambda_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mmin(eigenvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m overhead\n\u001b[1;32m     33\u001b[0m grids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(lambda_min, lambda_max, num\u001b[38;5;241m=\u001b[39mnum_bins)\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/numpy/core/fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "density, grids = density_generate(density[0], density[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.semilogy(grids, density + 1.0e-7)\n",
    "plt.ylabel('Density (Log Scale)', fontsize=14, labelpad=10)\n",
    "plt.xlabel('Eigenvlaue', fontsize=14, labelpad=10)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.axis([np.min(density[0]) - 1, np.max(density[0]) + 1, None, None])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.relu(nn.Dense(feat)(x))\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_loss(variables, batch, label):\n",
    "    output = model.apply({\"params\": variables}, batch)\n",
    "    loss = jnp.sum((output - label)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "model = MLP([8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "batch = jnp.ones((32, 10))\n",
    "label = jnp.ones((32))\n",
    "variables = model.init(jax.random.PRNGKey(0), batch)['params']\n",
    "f = lambda param: compute_loss(param, batch, label)\n",
    "loss = f(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': Array([    0.    ,  1357.4429,     0.    ,     0.    ,  2443.4028,\n",
       "             0.    ,  1695.0197, -2380.2268], dtype=float32),\n",
       "  'kernel': Array([[    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268]], dtype=float32)},\n",
       " 'Dense_1': {'bias': Array([3105.0737], dtype=float32),\n",
       "  'kernel': Array([[    0.    ],\n",
       "         [-8378.118 ],\n",
       "         [    0.    ],\n",
       "         [    0.    ],\n",
       "         [-6733.767 ],\n",
       "         [    0.    ],\n",
       "         [ 2216.5789],\n",
       "         [ 4594.799 ]], dtype=float32)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhessian.hvp import hvp, vmap_hvp\n",
    "from utils import normal_tree_like\n",
    "\n",
    "\n",
    "\n",
    "hvp(f, variables, normal_tree_like(rng, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = random.split(rng, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Hvs = vmap_hvp(f, variables, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hvs['Dense_0']['bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import forest_stack\n",
    "vs = forest_stack([normal_tree_like(r, variables) for  r in rngs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': Array([    0.    ,  1357.4429,     0.    ,     0.    ,  2443.4028,\n",
       "             0.    ,  1695.0197, -2380.2268], dtype=float32),\n",
       "  'kernel': Array([[    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268],\n",
       "         [    0.    ,  1357.4432,     0.    ,     0.    ,  2443.4023,\n",
       "              0.    ,  1695.0197, -2380.2268]], dtype=float32)},\n",
       " 'Dense_1': {'bias': Array([3105.0737], dtype=float32),\n",
       "  'kernel': Array([[    0.    ],\n",
       "         [-8378.118 ],\n",
       "         [    0.    ],\n",
       "         [    0.    ],\n",
       "         [-6733.767 ],\n",
       "         [    0.    ],\n",
       "         [ 2216.5789],\n",
       "         [ 4594.799 ]], dtype=float32)}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import normal_tree_like\n",
    "hvp(f, [variables, ], [normal_tree_like(rng, variables), ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jvp, grad\n",
    "\n",
    "# forward-over-reverse\n",
    "def hvp(f, primals, tangents):\n",
    "  return jvp(grad(f), primals, tangents)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian.hvp import hessian_vector_product\n",
    "from utils import rademacher_tree_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(33678.418, dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = rademacher_tree_like(random.PRNGKey(100), variables)\n",
    "Hv = hessian_vector_product(f, variables, v)\n",
    "utils.tree_inner_prod(Hv, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jvp, grad\n",
    "\n",
    "# forward-over-reverse\n",
    "def hvp(f, primals, tangents):\n",
    "  return jvp(grad(f), (primals, ) , (tangents, ))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(X):\n",
    "  return jnp.sum(jnp.tanh(X['val'])**2)\n",
    "\n",
    "key, subkey1, subkey2 = random.split(rng, 3)\n",
    "X = random.normal(subkey1, (30, 40))\n",
    "V = random.normal(subkey2, (30, 40))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10, ) + a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': Array([[ 0.06859969,  0.02169697, -0.04985059, ..., -0.01427879,\n",
       "          0.09185548, -0.13751534],\n",
       "        [ 0.16027963, -0.23671088,  0.32685554, ...,  0.83014745,\n",
       "          0.645875  ,  1.2392514 ],\n",
       "        [ 0.0720395 , -0.37492314,  0.4827116 , ...,  0.11219648,\n",
       "         -0.06260625,  0.06483959],\n",
       "        ...,\n",
       "        [-3.1227844 , -0.6699797 , -0.48910147, ..., -0.3941704 ,\n",
       "         -0.9792825 , -0.84505606],\n",
       "        [ 2.2148695 , -1.0342036 , -0.49956036, ...,  0.3227569 ,\n",
       "         -1.5029501 , -1.5642176 ],\n",
       "        [-0.02710684,  2.6805763 , -0.9925634 , ..., -0.09553632,\n",
       "         -1.8444042 , -0.36611918]], dtype=float32)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "primal and tangent arguments to jax.jvp do not match; dtypes must be equal, or in case of int/bool primal dtype the tangent dtype must be float0.Got primal dtype float32 and so expected tangent dtype float32, but got tangent dtype int32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Hv_ \u001b[38;5;241m=\u001b[39m \u001b[43mhvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 5\u001b[0m, in \u001b[0;36mhvp\u001b[0;34m(f, primals, tangents)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhvp\u001b[39m(f, primals, tangents):\n\u001b[0;32m----> 5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/api.py:1945\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(fun, primals, tangents, has_aux)\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes a (forward-mode) Jacobian-vector product of ``fun``.\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \n\u001b[1;32m   1909\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;124;03m0.19900084\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m check_callable(fun)\n\u001b[0;32m-> 1945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/api.py:1962\u001b[0m, in \u001b[0;36m_jvp\u001b[0;34m(fun, primals, tangents, has_aux)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, t \u001b[38;5;129;01min\u001b[39;00m safe_zip(ps_flat, ts_flat):\n\u001b[1;32m   1961\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m core\u001b[38;5;241m.\u001b[39mprimal_dtype_to_tangent_dtype(_dtype(p)) \u001b[38;5;241m!=\u001b[39m _dtype(t):\n\u001b[0;32m-> 1962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimal and tangent arguments to jax.jvp do not match; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes must be equal, or in case of int/bool primal dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1964\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe tangent dtype must be float0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1965\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot primal dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dtype(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and so expected tangent dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1966\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcore\u001b[38;5;241m.\u001b[39mprimal_dtype_to_tangent_dtype(_dtype(p))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1967\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtangent dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dtype(t)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1968\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(p) \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(t):\n\u001b[1;32m   1969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjvp called with different primal and tangent shapes;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot primal shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(p)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and tangent shape as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(t)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: primal and tangent arguments to jax.jvp do not match; dtypes must be equal, or in case of int/bool primal dtype the tangent dtype must be float0.Got primal dtype float32 and so expected tangent dtype float32, but got tangent dtype int32 instead."
     ]
    }
   ],
   "source": [
    "Hv_ = hvp(f, (variables, ) , (v, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': Array([    0.    , -1843.6448,     0.    ,     0.    , -1814.1292,\n",
       "             0.    , -1814.465 ,  1825.4143], dtype=float32),\n",
       "  'kernel': Array([[    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149],\n",
       "         [    0.    , -1843.6454,     0.    ,     0.    , -1814.1296,\n",
       "              0.    , -1814.4651,  1825.4149]], dtype=float32)},\n",
       " 'Dense_1': {'bias': Array([-175.96484], dtype=float32),\n",
       "  'kernel': Array([[    0.    ],\n",
       "         [ 8871.297 ],\n",
       "         [    0.    ],\n",
       "         [    0.    ],\n",
       "         [12588.34  ],\n",
       "         [    0.    ],\n",
       "         [ 1793.7302],\n",
       "         [-1955.8541]], dtype=float32)}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_reduce, tree_flatten, tree_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense_0': {'bias': {'Dense_0': {'bias': (8, 8), 'kernel': (8, 10, 8)}, 'Dense_1': {'bias': (8, 1), 'kernel': (8, 8, 1)}}, 'kernel': {'Dense_0': {'bias': (10, 8, 8), 'kernel': (10, 8, 10, 8)}, 'Dense_1': {'bias': (10, 8, 1), 'kernel': (10, 8, 8, 1)}}}, 'Dense_1': {'bias': {'Dense_0': {'bias': (1, 8), 'kernel': (1, 10, 8)}, 'Dense_1': {'bias': (1, 1), 'kernel': (1, 8, 1)}}, 'kernel': {'Dense_0': {'bias': (8, 1, 8), 'kernel': (8, 1, 10, 8)}, 'Dense_1': {'bias': (8, 1, 1), 'kernel': (8, 1, 8, 1)}}}}\n"
     ]
    }
   ],
   "source": [
    "print(jax.tree_map(jnp.shape, hessian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense_0': {'bias': (8,), 'kernel': (10, 8)}, 'Dense_1': {'bias': (1,), 'kernel': (8, 1)}}\n"
     ]
    }
   ],
   "source": [
    "print(jax.tree_map(jnp.shape, variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8, 10, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian['Dense_0']['kernel']['Dense_0']['kernel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected dict, got Array([[-1],\n       [ 1],\n       [ 1],\n       [-1],\n       [ 1],\n       [-1],\n       [ 1],\n       [-1]], dtype=int32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrademacher_tree_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m, in \u001b[0;36mhvp\u001b[0;34m(H, v)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhvp\u001b[39m(H, v):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/tree_util.py:243\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 243\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/condaenvs/jax-0.4.23/jax/_src/tree_util.py:243\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 243\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [\u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected dict, got Array([[-1],\n       [ 1],\n       [ 1],\n       [-1],\n       [ 1],\n       [-1],\n       [ 1],\n       [-1]], dtype=int32)."
     ]
    }
   ],
   "source": [
    "hessian_vector_product(f, variables, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import hessian, grad\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "def f(X):\n",
    "  return jnp.sum(jnp.tanh(X)**2)\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "X = random.normal(subkey1, (30, 40))\n",
    "V = random.normal(subkey2, (30, 40))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(478.88943, dtype=float32), Array(10.220588, dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(f, (X,), (V,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(442.30478, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.vdot(grad(f)(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(478.88943, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
